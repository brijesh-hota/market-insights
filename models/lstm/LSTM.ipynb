{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    "This notebook is used to run the LSTM model where we will predict the closing price of the next day for all the stocks present in the `ticker.csv` and finally predict with a new stock price and sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Installing `scikit-learn` and `tensorflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/site-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/site-packages (from tensorflow) (4.23.4)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow) (65.4.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.56.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importing libraries\n",
    "Following libraries are uploaded into the notebook here:\n",
    "*   `numpy` (imported as `np):\n",
    "    NumPy is used here for data pre-processing and assigning values to the train and test sets. NumPy is widely used in data analysis, numerical computations, and machine learning tasks.\n",
    "\n",
    "*   `pandas` (imported as `pd`):\n",
    "    With pandas, we have read and write data from various file formats, perform data cleaning, aggregation, filtering, and transformation tasks with ease.\n",
    "\n",
    "*   `sklearn.preprocessing.MinMaxScaler`:\n",
    "    MinMaxScaler is a data preprocessing class from the scikit-learn library (sklearn). It is used for feature scaling, specifically normalization. It transforms the data so that it lies within a specific range, typically [0, 1], by subtracting the minimum value and dividing by the range (maximum value - minimum value).\n",
    "\n",
    "*   `sklearn.metrics.mean_absolute_error`:\n",
    "    It calculates the mean absolute error between the true target values and predicted values. Mean absolute error is a measure of the average absolute difference between the predicted and actual values, and it provides an indication of how close the predictions are to the true values.\n",
    "\n",
    "*   `sklearn.metrics.mean_absolute_percentage_error`:\n",
    "    It calculates the mean absolute percentage error between the true target values and predicted values. Mean absolute percentage error is a measure of the average percentage difference between the predicted and actual values and provides insight into the accuracy of predictions relative to the true values.\n",
    "\n",
    "*   `tensorflow` (imported as `tf`):\n",
    "    TensorFlow provides a flexible framework for building, training, and deploying various machine learning models, especially deep learning models like Long Short-Term Memory (LSTM) networks. The code imports TensorFlow for building and training an LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import libraries for data preprocessing and evaluation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Import TensorFlow for building and training the LSTM model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for the model\n",
    "split = 0.85  # Split ratio for train-test data\n",
    "sequence_length = 10  # Length of input sequence for LSTM model\n",
    "epochs = 100  # Number of training epochs\n",
    "learning_rate = 0.02  # Learning rate for the optimizer\n",
    "\n",
    "# Load stock price and news data from CSV files\n",
    "stock_data = pd.read_csv(\"../fin-bert/finbert_stocks_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_stock_data: (638, 16)\n",
      "AMZN\n",
      "(109, 1)\n",
      "META\n",
      "(116, 1)\n",
      "AAPL\n",
      "(108, 1)\n",
      "MSFT\n",
      "(105, 1)\n",
      "TSLA\n",
      "(102, 1)\n",
      "Train & Test shape\n",
      "(540, 1)\n",
      "(98, 1)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for training and testing\n",
    "stock_column = ['Close']\n",
    "news_column = ['score']\n",
    "\n",
    "len_stock_data = stock_data.shape[0]\n",
    "print(\"len_stock_data: {}\".format(stock_data.shape))\n",
    "\n",
    "# Split data into training and testing sets\n",
    "\n",
    "ticker_df = pd.read_csv(\"../../ticker.csv\")\n",
    "ticker_list = ticker_df['ticker'].tolist()\n",
    "\n",
    "train_data = []\n",
    "train_sentiment_data = []\n",
    "test_data = []\n",
    "test_sentiment_data = []\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "\n",
    "for ticker in ticker_list:\n",
    "    filtered_stock = stock_data[stock_data['related'] == ticker]\n",
    "    print(ticker)\n",
    "    train_examples = int(filtered_stock.shape[0] * split)\n",
    "    train_temp = filtered_stock[stock_column].values[:train_examples]\n",
    "    train_sentiment_temp = filtered_stock[news_column].values[:train_examples]\n",
    "    test_temp = filtered_stock[stock_column].values[train_examples:]\n",
    "    test_sentiment_temp = filtered_stock[news_column].values[train_examples:]\n",
    "\n",
    "    # To store the dates\n",
    "    test_df = filtered_stock[['related', 'datetime_norm']].copy()\n",
    "\n",
    "    train_data.extend(train_temp)\n",
    "    train_sentiment_data.extend(train_sentiment_temp)\n",
    "    test_data.extend(test_temp)\n",
    "    test_sentiment_data.extend(test_sentiment_temp)\n",
    "\n",
    "# Convert Python lists to NumPy arrays\n",
    "train = np.array(train_data)\n",
    "train_sentiment = np.array(train_sentiment_data)\n",
    "test = np.array(test_data)\n",
    "test_sentiment = np.array(test_sentiment_data)\n",
    "\n",
    "len_train = train.shape[0]\n",
    "len_test = test.shape[0]\n",
    "len_train_sentiment = train_sentiment.shape[0]\n",
    "len_test_sentiment = test_sentiment.shape[0]\n",
    "\n",
    "# Reshape train and test arrays to 2D\n",
    "train = train.reshape(-1, 1)\n",
    "test = test.reshape(-1, 1)\n",
    "\n",
    "print(\"Train & Test shape\")\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train, test = scaler.fit_transform(train), scaler.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input features (X) and target values (y) for training data\n",
    "X_train = []\n",
    "for i in range(len_train - sequence_length):\n",
    "    X_train.append(train[i: i + sequence_length])\n",
    "len_X_train = len(X_train)\n",
    "y_train = np.array(train[sequence_length:]).astype(float)\n",
    "# print(X_train)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input features (X) and target values (y) for testing data\n",
    "X_test = []\n",
    "for i in range(len_test - sequence_length):\n",
    "    X_test.append(test[i: i + sequence_length])\n",
    "len_X_test = len(X_test)\n",
    "y_test = np.array(test[sequence_length:]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.010189436614223446], [0.007623204081348867], [0.0], [0.011170650792412695], [0.016001199373743558], [0.025473620327899293], [0.04517319586257518], [0.04585249355634613], [0.05660804517310952], [0.04879613609091393], [-0.97]]\n",
      "[[0.009904007917537538], [0.006294569941466488], [0.005634260208446595], [0.005018011423942759], [0.0], [0.010828414677333309], [0.010916469408285279], [0.002024822232375545], [0.008275364808365993], [0.003873568585887055], [0.0]]\n"
     ]
    }
   ],
   "source": [
    "# Add news sentiment to the input features (X) for both training and testing data\n",
    "for i in range(len_X_train):\n",
    "    X_train[i] = X_train[i].tolist()\n",
    "    X_train[i].append(train_sentiment[sequence_length + i].tolist())\n",
    "    if i == 0:\n",
    "        print(X_train[i])\n",
    "X_train = np.array(X_train).astype(float)\n",
    "\n",
    "for i in range(len_X_test):\n",
    "    X_test[i] = X_test[i].tolist()\n",
    "    X_test[i].append(test_sentiment[sequence_length + i].tolist())\n",
    "    if i == 0:\n",
    "        print(X_test[i])\n",
    "X_test = np.array(X_test).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (530, 11, 1)\n",
      "X_test: (88, 11, 1)\n",
      "y_train: (530, 1)\n",
      "y_test: (88, 1)\n",
      "[[[ 0.00990401]\n",
      "  [ 0.00629457]\n",
      "  [ 0.00563426]\n",
      "  [ 0.00501801]\n",
      "  [ 0.        ]\n",
      "  [ 0.01082841]\n",
      "  [ 0.01091647]\n",
      "  [ 0.00202482]\n",
      "  [ 0.00827536]\n",
      "  [ 0.00387357]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.00629457]\n",
      "  [ 0.00563426]\n",
      "  [ 0.00501801]\n",
      "  [ 0.        ]\n",
      "  [ 0.01082841]\n",
      "  [ 0.01091647]\n",
      "  [ 0.00202482]\n",
      "  [ 0.00827536]\n",
      "  [ 0.00387357]\n",
      "  [ 0.01276522]\n",
      "  [-0.964     ]]\n",
      "\n",
      " [[ 0.00563426]\n",
      "  [ 0.00501801]\n",
      "  [ 0.        ]\n",
      "  [ 0.01082841]\n",
      "  [ 0.01091647]\n",
      "  [ 0.00202482]\n",
      "  [ 0.00827536]\n",
      "  [ 0.00387357]\n",
      "  [ 0.01276522]\n",
      "  [ 0.0281715 ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.00501801]\n",
      "  [ 0.        ]\n",
      "  [ 0.01082841]\n",
      "  [ 0.01091647]\n",
      "  [ 0.00202482]\n",
      "  [ 0.00827536]\n",
      "  [ 0.00387357]\n",
      "  [ 0.01276522]\n",
      "  [ 0.0281715 ]\n",
      "  [ 0.02984414]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.01082841]\n",
      "  [ 0.01091647]\n",
      "  [ 0.00202482]\n",
      "  [ 0.00827536]\n",
      "  [ 0.00387357]\n",
      "  [ 0.01276522]\n",
      "  [ 0.0281715 ]\n",
      "  [ 0.02984414]\n",
      "  [ 0.02170086]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.01082841]\n",
      "  [ 0.01091647]\n",
      "  [ 0.00202482]\n",
      "  [ 0.00827536]\n",
      "  [ 0.00387357]\n",
      "  [ 0.01276522]\n",
      "  [ 0.0281715 ]\n",
      "  [ 0.02984414]\n",
      "  [ 0.02170086]\n",
      "  [ 0.0328374 ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.01091647]\n",
      "  [ 0.00202482]\n",
      "  [ 0.00827536]\n",
      "  [ 0.00387357]\n",
      "  [ 0.01276522]\n",
      "  [ 0.0281715 ]\n",
      "  [ 0.02984414]\n",
      "  [ 0.02170086]\n",
      "  [ 0.0328374 ]\n",
      "  [ 0.00906772]\n",
      "  [ 0.813     ]]\n",
      "\n",
      " [[ 0.00202482]\n",
      "  [ 0.00827536]\n",
      "  [ 0.00387357]\n",
      "  [ 0.01276522]\n",
      "  [ 0.0281715 ]\n",
      "  [ 0.02984414]\n",
      "  [ 0.02170086]\n",
      "  [ 0.0328374 ]\n",
      "  [ 0.00906772]\n",
      "  [ 0.00924377]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.00827536]\n",
      "  [ 0.00387357]\n",
      "  [ 0.01276522]\n",
      "  [ 0.0281715 ]\n",
      "  [ 0.02984414]\n",
      "  [ 0.02170086]\n",
      "  [ 0.0328374 ]\n",
      "  [ 0.00906772]\n",
      "  [ 0.00924377]\n",
      "  [ 0.00396162]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.00387357]\n",
      "  [ 0.01276522]\n",
      "  [ 0.0281715 ]\n",
      "  [ 0.02984414]\n",
      "  [ 0.02170086]\n",
      "  [ 0.0328374 ]\n",
      "  [ 0.00906772]\n",
      "  [ 0.00924377]\n",
      "  [ 0.00396162]\n",
      "  [ 0.00541422]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.01276522]\n",
      "  [ 0.0281715 ]\n",
      "  [ 0.02984414]\n",
      "  [ 0.02170086]\n",
      "  [ 0.0328374 ]\n",
      "  [ 0.00906772]\n",
      "  [ 0.00924377]\n",
      "  [ 0.00396162]\n",
      "  [ 0.00541422]\n",
      "  [ 0.00110042]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.0281715 ]\n",
      "  [ 0.02984414]\n",
      "  [ 0.02170086]\n",
      "  [ 0.0328374 ]\n",
      "  [ 0.00906772]\n",
      "  [ 0.00924377]\n",
      "  [ 0.00396162]\n",
      "  [ 0.00541422]\n",
      "  [ 0.00110042]\n",
      "  [ 0.69099399]\n",
      "  [ 0.79      ]]\n",
      "\n",
      " [[ 0.02984414]\n",
      "  [ 0.02170086]\n",
      "  [ 0.0328374 ]\n",
      "  [ 0.00906772]\n",
      "  [ 0.00924377]\n",
      "  [ 0.00396162]\n",
      "  [ 0.00541422]\n",
      "  [ 0.00110042]\n",
      "  [ 0.69099399]\n",
      "  [ 0.70794093]\n",
      "  [-0.917     ]]\n",
      "\n",
      " [[ 0.02170086]\n",
      "  [ 0.0328374 ]\n",
      "  [ 0.00906772]\n",
      "  [ 0.00924377]\n",
      "  [ 0.00396162]\n",
      "  [ 0.00541422]\n",
      "  [ 0.00110042]\n",
      "  [ 0.69099399]\n",
      "  [ 0.70794093]\n",
      "  [ 0.66277846]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.0328374 ]\n",
      "  [ 0.00906772]\n",
      "  [ 0.00924377]\n",
      "  [ 0.00396162]\n",
      "  [ 0.00541422]\n",
      "  [ 0.00110042]\n",
      "  [ 0.69099399]\n",
      "  [ 0.70794093]\n",
      "  [ 0.66277846]\n",
      "  [ 0.70054581]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.00906772]\n",
      "  [ 0.00924377]\n",
      "  [ 0.00396162]\n",
      "  [ 0.00541422]\n",
      "  [ 0.00110042]\n",
      "  [ 0.69099399]\n",
      "  [ 0.70794093]\n",
      "  [ 0.66277846]\n",
      "  [ 0.70054581]\n",
      "  [ 0.69279874]\n",
      "  [-0.958     ]]\n",
      "\n",
      " [[ 0.00924377]\n",
      "  [ 0.00396162]\n",
      "  [ 0.00541422]\n",
      "  [ 0.00110042]\n",
      "  [ 0.69099399]\n",
      "  [ 0.70794093]\n",
      "  [ 0.66277846]\n",
      "  [ 0.70054581]\n",
      "  [ 0.69279874]\n",
      "  [ 0.67624794]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.00396162]\n",
      "  [ 0.00541422]\n",
      "  [ 0.00110042]\n",
      "  [ 0.69099399]\n",
      "  [ 0.70794093]\n",
      "  [ 0.66277846]\n",
      "  [ 0.70054581]\n",
      "  [ 0.69279874]\n",
      "  [ 0.67624794]\n",
      "  [ 0.70023778]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.00541422]\n",
      "  [ 0.00110042]\n",
      "  [ 0.69099399]\n",
      "  [ 0.70794093]\n",
      "  [ 0.66277846]\n",
      "  [ 0.70054581]\n",
      "  [ 0.69279874]\n",
      "  [ 0.67624794]\n",
      "  [ 0.70023778]\n",
      "  [ 0.73276699]\n",
      "  [-0.964     ]]\n",
      "\n",
      " [[ 0.00110042]\n",
      "  [ 0.69099399]\n",
      "  [ 0.70794093]\n",
      "  [ 0.66277846]\n",
      "  [ 0.70054581]\n",
      "  [ 0.69279874]\n",
      "  [ 0.67624794]\n",
      "  [ 0.70023778]\n",
      "  [ 0.73276699]\n",
      "  [ 0.72229069]\n",
      "  [-0.865     ]]\n",
      "\n",
      " [[ 0.69099399]\n",
      "  [ 0.70794093]\n",
      "  [ 0.66277846]\n",
      "  [ 0.70054581]\n",
      "  [ 0.69279874]\n",
      "  [ 0.67624794]\n",
      "  [ 0.70023778]\n",
      "  [ 0.73276699]\n",
      "  [ 0.72229069]\n",
      "  [ 0.71586411]\n",
      "  [-0.656     ]]\n",
      "\n",
      " [[ 0.70794093]\n",
      "  [ 0.66277846]\n",
      "  [ 0.70054581]\n",
      "  [ 0.69279874]\n",
      "  [ 0.67624794]\n",
      "  [ 0.70023778]\n",
      "  [ 0.73276699]\n",
      "  [ 0.72229069]\n",
      "  [ 0.71586411]\n",
      "  [ 0.75002209]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.66277846]\n",
      "  [ 0.70054581]\n",
      "  [ 0.69279874]\n",
      "  [ 0.67624794]\n",
      "  [ 0.70023778]\n",
      "  [ 0.73276699]\n",
      "  [ 0.72229069]\n",
      "  [ 0.71586411]\n",
      "  [ 0.75002209]\n",
      "  [ 0.79866188]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.70054581]\n",
      "  [ 0.69279874]\n",
      "  [ 0.67624794]\n",
      "  [ 0.70023778]\n",
      "  [ 0.73276699]\n",
      "  [ 0.72229069]\n",
      "  [ 0.71586411]\n",
      "  [ 0.75002209]\n",
      "  [ 0.79866188]\n",
      "  [ 0.81657723]\n",
      "  [-0.953     ]]\n",
      "\n",
      " [[ 0.69279874]\n",
      "  [ 0.67624794]\n",
      "  [ 0.70023778]\n",
      "  [ 0.73276699]\n",
      "  [ 0.72229069]\n",
      "  [ 0.71586411]\n",
      "  [ 0.75002209]\n",
      "  [ 0.79866188]\n",
      "  [ 0.81657723]\n",
      "  [ 0.79659303]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.67624794]\n",
      "  [ 0.70023778]\n",
      "  [ 0.73276699]\n",
      "  [ 0.72229069]\n",
      "  [ 0.71586411]\n",
      "  [ 0.75002209]\n",
      "  [ 0.79866188]\n",
      "  [ 0.81657723]\n",
      "  [ 0.79659303]\n",
      "  [ 0.81059071]\n",
      "  [-0.941     ]]\n",
      "\n",
      " [[ 0.70023778]\n",
      "  [ 0.73276699]\n",
      "  [ 0.72229069]\n",
      "  [ 0.71586411]\n",
      "  [ 0.75002209]\n",
      "  [ 0.79866188]\n",
      "  [ 0.81657723]\n",
      "  [ 0.79659303]\n",
      "  [ 0.81059071]\n",
      "  [ 0.82802192]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.73276699]\n",
      "  [ 0.72229069]\n",
      "  [ 0.71586411]\n",
      "  [ 0.75002209]\n",
      "  [ 0.79866188]\n",
      "  [ 0.81657723]\n",
      "  [ 0.79659303]\n",
      "  [ 0.81059071]\n",
      "  [ 0.82802192]\n",
      "  [ 0.7686416 ]\n",
      "  [ 0.813     ]]\n",
      "\n",
      " [[ 0.72229069]\n",
      "  [ 0.71586411]\n",
      "  [ 0.75002209]\n",
      "  [ 0.79866188]\n",
      "  [ 0.81657723]\n",
      "  [ 0.79659303]\n",
      "  [ 0.81059071]\n",
      "  [ 0.82802192]\n",
      "  [ 0.7686416 ]\n",
      "  [ 0.73228286]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.71586411]\n",
      "  [ 0.75002209]\n",
      "  [ 0.79866188]\n",
      "  [ 0.81657723]\n",
      "  [ 0.79659303]\n",
      "  [ 0.81059071]\n",
      "  [ 0.82802192]\n",
      "  [ 0.7686416 ]\n",
      "  [ 0.73228286]\n",
      "  [ 0.72061799]\n",
      "  [ 0.743     ]]\n",
      "\n",
      " [[ 0.75002209]\n",
      "  [ 0.79866188]\n",
      "  [ 0.81657723]\n",
      "  [ 0.79659303]\n",
      "  [ 0.81059071]\n",
      "  [ 0.82802192]\n",
      "  [ 0.7686416 ]\n",
      "  [ 0.73228286]\n",
      "  [ 0.72061799]\n",
      "  [ 0.7332072 ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.79866188]\n",
      "  [ 0.81657723]\n",
      "  [ 0.79659303]\n",
      "  [ 0.81059071]\n",
      "  [ 0.82802192]\n",
      "  [ 0.7686416 ]\n",
      "  [ 0.73228286]\n",
      "  [ 0.72061799]\n",
      "  [ 0.7332072 ]\n",
      "  [ 0.75125459]\n",
      "  [-0.635     ]]\n",
      "\n",
      " [[ 0.81657723]\n",
      "  [ 0.79659303]\n",
      "  [ 0.81059071]\n",
      "  [ 0.82802192]\n",
      "  [ 0.7686416 ]\n",
      "  [ 0.73228286]\n",
      "  [ 0.72061799]\n",
      "  [ 0.7332072 ]\n",
      "  [ 0.75125459]\n",
      "  [ 0.24676472]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.79659303]\n",
      "  [ 0.81059071]\n",
      "  [ 0.82802192]\n",
      "  [ 0.7686416 ]\n",
      "  [ 0.73228286]\n",
      "  [ 0.72061799]\n",
      "  [ 0.7332072 ]\n",
      "  [ 0.75125459]\n",
      "  [ 0.24676472]\n",
      "  [ 0.26014615]\n",
      "  [-0.963     ]]\n",
      "\n",
      " [[ 0.81059071]\n",
      "  [ 0.82802192]\n",
      "  [ 0.7686416 ]\n",
      "  [ 0.73228286]\n",
      "  [ 0.72061799]\n",
      "  [ 0.7332072 ]\n",
      "  [ 0.75125459]\n",
      "  [ 0.24676472]\n",
      "  [ 0.26014615]\n",
      "  [ 0.25873754]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.82802192]\n",
      "  [ 0.7686416 ]\n",
      "  [ 0.73228286]\n",
      "  [ 0.72061799]\n",
      "  [ 0.7332072 ]\n",
      "  [ 0.75125459]\n",
      "  [ 0.24676472]\n",
      "  [ 0.26014615]\n",
      "  [ 0.25873754]\n",
      "  [ 0.27005019]\n",
      "  [ 0.633     ]]\n",
      "\n",
      " [[ 0.7686416 ]\n",
      "  [ 0.73228286]\n",
      "  [ 0.72061799]\n",
      "  [ 0.7332072 ]\n",
      "  [ 0.75125459]\n",
      "  [ 0.24676472]\n",
      "  [ 0.26014615]\n",
      "  [ 0.25873754]\n",
      "  [ 0.27005019]\n",
      "  [ 0.27154679]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.73228286]\n",
      "  [ 0.72061799]\n",
      "  [ 0.7332072 ]\n",
      "  [ 0.75125459]\n",
      "  [ 0.24676472]\n",
      "  [ 0.26014615]\n",
      "  [ 0.25873754]\n",
      "  [ 0.27005019]\n",
      "  [ 0.27154679]\n",
      "  [ 0.29082668]\n",
      "  [-0.693     ]]\n",
      "\n",
      " [[ 0.72061799]\n",
      "  [ 0.7332072 ]\n",
      "  [ 0.75125459]\n",
      "  [ 0.24676472]\n",
      "  [ 0.26014615]\n",
      "  [ 0.25873754]\n",
      "  [ 0.27005019]\n",
      "  [ 0.27154679]\n",
      "  [ 0.29082668]\n",
      "  [ 0.28417999]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.7332072 ]\n",
      "  [ 0.75125459]\n",
      "  [ 0.24676472]\n",
      "  [ 0.26014615]\n",
      "  [ 0.25873754]\n",
      "  [ 0.27005019]\n",
      "  [ 0.27154679]\n",
      "  [ 0.29082668]\n",
      "  [ 0.28417999]\n",
      "  [ 0.27920594]\n",
      "  [ 0.506     ]]\n",
      "\n",
      " [[ 0.75125459]\n",
      "  [ 0.24676472]\n",
      "  [ 0.26014615]\n",
      "  [ 0.25873754]\n",
      "  [ 0.27005019]\n",
      "  [ 0.27154679]\n",
      "  [ 0.29082668]\n",
      "  [ 0.28417999]\n",
      "  [ 0.27920594]\n",
      "  [ 0.28131878]\n",
      "  [-0.965     ]]\n",
      "\n",
      " [[ 0.24676472]\n",
      "  [ 0.26014615]\n",
      "  [ 0.25873754]\n",
      "  [ 0.27005019]\n",
      "  [ 0.27154679]\n",
      "  [ 0.29082668]\n",
      "  [ 0.28417999]\n",
      "  [ 0.27920594]\n",
      "  [ 0.28131878]\n",
      "  [ 0.27634473]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.26014615]\n",
      "  [ 0.25873754]\n",
      "  [ 0.27005019]\n",
      "  [ 0.27154679]\n",
      "  [ 0.29082668]\n",
      "  [ 0.28417999]\n",
      "  [ 0.27920594]\n",
      "  [ 0.28131878]\n",
      "  [ 0.27634473]\n",
      "  [ 0.2649001 ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.25873754]\n",
      "  [ 0.27005019]\n",
      "  [ 0.27154679]\n",
      "  [ 0.29082668]\n",
      "  [ 0.28417999]\n",
      "  [ 0.27920594]\n",
      "  [ 0.28131878]\n",
      "  [ 0.27634473]\n",
      "  [ 0.2649001 ]\n",
      "  [ 0.27233914]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.27005019]\n",
      "  [ 0.27154679]\n",
      "  [ 0.29082668]\n",
      "  [ 0.28417999]\n",
      "  [ 0.27920594]\n",
      "  [ 0.28131878]\n",
      "  [ 0.27634473]\n",
      "  [ 0.2649001 ]\n",
      "  [ 0.27233914]\n",
      "  [ 0.27572848]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.27154679]\n",
      "  [ 0.29082668]\n",
      "  [ 0.28417999]\n",
      "  [ 0.27920594]\n",
      "  [ 0.28131878]\n",
      "  [ 0.27634473]\n",
      "  [ 0.2649001 ]\n",
      "  [ 0.27233914]\n",
      "  [ 0.27572848]\n",
      "  [ 0.27638879]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.29082668]\n",
      "  [ 0.28417999]\n",
      "  [ 0.27920594]\n",
      "  [ 0.28131878]\n",
      "  [ 0.27634473]\n",
      "  [ 0.2649001 ]\n",
      "  [ 0.27233914]\n",
      "  [ 0.27572848]\n",
      "  [ 0.27638879]\n",
      "  [ 0.29580073]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.28417999]\n",
      "  [ 0.27920594]\n",
      "  [ 0.28131878]\n",
      "  [ 0.27634473]\n",
      "  [ 0.2649001 ]\n",
      "  [ 0.27233914]\n",
      "  [ 0.27572848]\n",
      "  [ 0.27638879]\n",
      "  [ 0.29580073]\n",
      "  [ 0.28712918]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.27920594]\n",
      "  [ 0.28131878]\n",
      "  [ 0.27634473]\n",
      "  [ 0.2649001 ]\n",
      "  [ 0.27233914]\n",
      "  [ 0.27572848]\n",
      "  [ 0.27638879]\n",
      "  [ 0.29580073]\n",
      "  [ 0.28712918]\n",
      "  [ 0.28189103]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.28131878]\n",
      "  [ 0.27634473]\n",
      "  [ 0.2649001 ]\n",
      "  [ 0.27233914]\n",
      "  [ 0.27572848]\n",
      "  [ 0.27638879]\n",
      "  [ 0.29580073]\n",
      "  [ 0.28712918]\n",
      "  [ 0.28189103]\n",
      "  [ 0.28545648]\n",
      "  [ 0.673     ]]\n",
      "\n",
      " [[ 0.27634473]\n",
      "  [ 0.2649001 ]\n",
      "  [ 0.27233914]\n",
      "  [ 0.27572848]\n",
      "  [ 0.27638879]\n",
      "  [ 0.29580073]\n",
      "  [ 0.28712918]\n",
      "  [ 0.28189103]\n",
      "  [ 0.28545648]\n",
      "  [ 0.28928602]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.2649001 ]\n",
      "  [ 0.27233914]\n",
      "  [ 0.27572848]\n",
      "  [ 0.27638879]\n",
      "  [ 0.29580073]\n",
      "  [ 0.28712918]\n",
      "  [ 0.28189103]\n",
      "  [ 0.28545648]\n",
      "  [ 0.28928602]\n",
      "  [ 0.29315962]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.27233914]\n",
      "  [ 0.27572848]\n",
      "  [ 0.27638879]\n",
      "  [ 0.29580073]\n",
      "  [ 0.28712918]\n",
      "  [ 0.28189103]\n",
      "  [ 0.28545648]\n",
      "  [ 0.28928602]\n",
      "  [ 0.29315962]\n",
      "  [ 0.92503742]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.27572848]\n",
      "  [ 0.27638879]\n",
      "  [ 0.29580073]\n",
      "  [ 0.28712918]\n",
      "  [ 0.28189103]\n",
      "  [ 0.28545648]\n",
      "  [ 0.28928602]\n",
      "  [ 0.29315962]\n",
      "  [ 0.92503742]\n",
      "  [ 0.90527339]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.27638879]\n",
      "  [ 0.29580073]\n",
      "  [ 0.28712918]\n",
      "  [ 0.28189103]\n",
      "  [ 0.28545648]\n",
      "  [ 0.28928602]\n",
      "  [ 0.29315962]\n",
      "  [ 0.92503742]\n",
      "  [ 0.90527339]\n",
      "  [ 0.93234441]\n",
      "  [-0.929     ]]\n",
      "\n",
      " [[ 0.29580073]\n",
      "  [ 0.28712918]\n",
      "  [ 0.28189103]\n",
      "  [ 0.28545648]\n",
      "  [ 0.28928602]\n",
      "  [ 0.29315962]\n",
      "  [ 0.92503742]\n",
      "  [ 0.90527339]\n",
      "  [ 0.93234441]\n",
      "  [ 0.91169998]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.28712918]\n",
      "  [ 0.28189103]\n",
      "  [ 0.28545648]\n",
      "  [ 0.28928602]\n",
      "  [ 0.29315962]\n",
      "  [ 0.92503742]\n",
      "  [ 0.90527339]\n",
      "  [ 0.93234441]\n",
      "  [ 0.91169998]\n",
      "  [ 0.91535354]\n",
      "  [ 0.922     ]]\n",
      "\n",
      " [[ 0.28189103]\n",
      "  [ 0.28545648]\n",
      "  [ 0.28928602]\n",
      "  [ 0.29315962]\n",
      "  [ 0.92503742]\n",
      "  [ 0.90527339]\n",
      "  [ 0.93234441]\n",
      "  [ 0.91169998]\n",
      "  [ 0.91535354]\n",
      "  [ 0.91183203]\n",
      "  [ 0.493     ]]\n",
      "\n",
      " [[ 0.28545648]\n",
      "  [ 0.28928602]\n",
      "  [ 0.29315962]\n",
      "  [ 0.92503742]\n",
      "  [ 0.90527339]\n",
      "  [ 0.93234441]\n",
      "  [ 0.91169998]\n",
      "  [ 0.91535354]\n",
      "  [ 0.91183203]\n",
      "  [ 0.93599798]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.28928602]\n",
      "  [ 0.29315962]\n",
      "  [ 0.92503742]\n",
      "  [ 0.90527339]\n",
      "  [ 0.93234441]\n",
      "  [ 0.91169998]\n",
      "  [ 0.91535354]\n",
      "  [ 0.91183203]\n",
      "  [ 0.93599798]\n",
      "  [ 0.92477332]\n",
      "  [ 0.866     ]]\n",
      "\n",
      " [[ 0.29315962]\n",
      "  [ 0.92503742]\n",
      "  [ 0.90527339]\n",
      "  [ 0.93234441]\n",
      "  [ 0.91169998]\n",
      "  [ 0.91535354]\n",
      "  [ 0.91183203]\n",
      "  [ 0.93599798]\n",
      "  [ 0.92477332]\n",
      "  [ 0.92547762]\n",
      "  [ 0.506     ]]\n",
      "\n",
      " [[ 0.92503742]\n",
      "  [ 0.90527339]\n",
      "  [ 0.93234441]\n",
      "  [ 0.91169998]\n",
      "  [ 0.91535354]\n",
      "  [ 0.91183203]\n",
      "  [ 0.93599798]\n",
      "  [ 0.92477332]\n",
      "  [ 0.92547762]\n",
      "  [ 0.9392112 ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.90527339]\n",
      "  [ 0.93234441]\n",
      "  [ 0.91169998]\n",
      "  [ 0.91535354]\n",
      "  [ 0.91183203]\n",
      "  [ 0.93599798]\n",
      "  [ 0.92477332]\n",
      "  [ 0.92547762]\n",
      "  [ 0.9392112 ]\n",
      "  [ 0.92138398]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.93234441]\n",
      "  [ 0.91169998]\n",
      "  [ 0.91535354]\n",
      "  [ 0.91183203]\n",
      "  [ 0.93599798]\n",
      "  [ 0.92477332]\n",
      "  [ 0.92547762]\n",
      "  [ 0.9392112 ]\n",
      "  [ 0.92138398]\n",
      "  [ 0.94532977]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.91169998]\n",
      "  [ 0.91535354]\n",
      "  [ 0.91183203]\n",
      "  [ 0.93599798]\n",
      "  [ 0.92477332]\n",
      "  [ 0.92547762]\n",
      "  [ 0.9392112 ]\n",
      "  [ 0.92138398]\n",
      "  [ 0.94532977]\n",
      "  [ 0.95668634]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.91535354]\n",
      "  [ 0.91183203]\n",
      "  [ 0.93599798]\n",
      "  [ 0.92477332]\n",
      "  [ 0.92547762]\n",
      "  [ 0.9392112 ]\n",
      "  [ 0.92138398]\n",
      "  [ 0.94532977]\n",
      "  [ 0.95668634]\n",
      "  [ 1.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.91183203]\n",
      "  [ 0.93599798]\n",
      "  [ 0.92477332]\n",
      "  [ 0.92547762]\n",
      "  [ 0.9392112 ]\n",
      "  [ 0.92138398]\n",
      "  [ 0.94532977]\n",
      "  [ 0.95668634]\n",
      "  [ 1.        ]\n",
      "  [ 0.96386129]\n",
      "  [-0.814     ]]\n",
      "\n",
      " [[ 0.93599798]\n",
      "  [ 0.92477332]\n",
      "  [ 0.92547762]\n",
      "  [ 0.9392112 ]\n",
      "  [ 0.92138398]\n",
      "  [ 0.94532977]\n",
      "  [ 0.95668634]\n",
      "  [ 1.        ]\n",
      "  [ 0.96386129]\n",
      "  [ 0.9502157 ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.92477332]\n",
      "  [ 0.92547762]\n",
      "  [ 0.9392112 ]\n",
      "  [ 0.92138398]\n",
      "  [ 0.94532977]\n",
      "  [ 0.95668634]\n",
      "  [ 1.        ]\n",
      "  [ 0.96386129]\n",
      "  [ 0.9502157 ]\n",
      "  [ 0.95611409]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.92547762]\n",
      "  [ 0.9392112 ]\n",
      "  [ 0.92138398]\n",
      "  [ 0.94532977]\n",
      "  [ 0.95668634]\n",
      "  [ 1.        ]\n",
      "  [ 0.96386129]\n",
      "  [ 0.9502157 ]\n",
      "  [ 0.95611409]\n",
      "  [ 0.98195274]\n",
      "  [ 0.925     ]]\n",
      "\n",
      " [[ 0.9392112 ]\n",
      "  [ 0.92138398]\n",
      "  [ 0.94532977]\n",
      "  [ 0.95668634]\n",
      "  [ 1.        ]\n",
      "  [ 0.96386129]\n",
      "  [ 0.9502157 ]\n",
      "  [ 0.95611409]\n",
      "  [ 0.98195274]\n",
      "  [ 0.92380492]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.92138398]\n",
      "  [ 0.94532977]\n",
      "  [ 0.95668634]\n",
      "  [ 1.        ]\n",
      "  [ 0.96386129]\n",
      "  [ 0.9502157 ]\n",
      "  [ 0.95611409]\n",
      "  [ 0.98195274]\n",
      "  [ 0.92380492]\n",
      "  [ 0.57910027]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.94532977]\n",
      "  [ 0.95668634]\n",
      "  [ 1.        ]\n",
      "  [ 0.96386129]\n",
      "  [ 0.9502157 ]\n",
      "  [ 0.95611409]\n",
      "  [ 0.98195274]\n",
      "  [ 0.92380492]\n",
      "  [ 0.57910027]\n",
      "  [ 0.60176949]\n",
      "  [ 0.555     ]]\n",
      "\n",
      " [[ 0.95668634]\n",
      "  [ 1.        ]\n",
      "  [ 0.96386129]\n",
      "  [ 0.9502157 ]\n",
      "  [ 0.95611409]\n",
      "  [ 0.98195274]\n",
      "  [ 0.92380492]\n",
      "  [ 0.57910027]\n",
      "  [ 0.60176949]\n",
      "  [ 0.56651119]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 1.        ]\n",
      "  [ 0.96386129]\n",
      "  [ 0.9502157 ]\n",
      "  [ 0.95611409]\n",
      "  [ 0.98195274]\n",
      "  [ 0.92380492]\n",
      "  [ 0.57910027]\n",
      "  [ 0.60176949]\n",
      "  [ 0.56651119]\n",
      "  [ 0.53838372]\n",
      "  [-0.889     ]]\n",
      "\n",
      " [[ 0.96386129]\n",
      "  [ 0.9502157 ]\n",
      "  [ 0.95611409]\n",
      "  [ 0.98195274]\n",
      "  [ 0.92380492]\n",
      "  [ 0.57910027]\n",
      "  [ 0.60176949]\n",
      "  [ 0.56651119]\n",
      "  [ 0.53838372]\n",
      "  [ 0.56492648]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.9502157 ]\n",
      "  [ 0.95611409]\n",
      "  [ 0.98195274]\n",
      "  [ 0.92380492]\n",
      "  [ 0.57910027]\n",
      "  [ 0.60176949]\n",
      "  [ 0.56651119]\n",
      "  [ 0.53838372]\n",
      "  [ 0.56492648]\n",
      "  [ 0.57047278]\n",
      "  [-0.759     ]]\n",
      "\n",
      " [[ 0.95611409]\n",
      "  [ 0.98195274]\n",
      "  [ 0.92380492]\n",
      "  [ 0.57910027]\n",
      "  [ 0.60176949]\n",
      "  [ 0.56651119]\n",
      "  [ 0.53838372]\n",
      "  [ 0.56492648]\n",
      "  [ 0.57047278]\n",
      "  [ 0.58926841]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.98195274]\n",
      "  [ 0.92380492]\n",
      "  [ 0.57910027]\n",
      "  [ 0.60176949]\n",
      "  [ 0.56651119]\n",
      "  [ 0.53838372]\n",
      "  [ 0.56492648]\n",
      "  [ 0.57047278]\n",
      "  [ 0.58926841]\n",
      "  [ 0.6804297 ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.92380492]\n",
      "  [ 0.57910027]\n",
      "  [ 0.60176949]\n",
      "  [ 0.56651119]\n",
      "  [ 0.53838372]\n",
      "  [ 0.56492648]\n",
      "  [ 0.57047278]\n",
      "  [ 0.58926841]\n",
      "  [ 0.6804297 ]\n",
      "  [ 0.65428302]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.57910027]\n",
      "  [ 0.60176949]\n",
      "  [ 0.56651119]\n",
      "  [ 0.53838372]\n",
      "  [ 0.56492648]\n",
      "  [ 0.57047278]\n",
      "  [ 0.58926841]\n",
      "  [ 0.6804297 ]\n",
      "  [ 0.65428302]\n",
      "  [ 0.64499516]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.60176949]\n",
      "  [ 0.56651119]\n",
      "  [ 0.53838372]\n",
      "  [ 0.56492648]\n",
      "  [ 0.57047278]\n",
      "  [ 0.58926841]\n",
      "  [ 0.6804297 ]\n",
      "  [ 0.65428302]\n",
      "  [ 0.64499516]\n",
      "  [ 0.63425477]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.56651119]\n",
      "  [ 0.53838372]\n",
      "  [ 0.56492648]\n",
      "  [ 0.57047278]\n",
      "  [ 0.58926841]\n",
      "  [ 0.6804297 ]\n",
      "  [ 0.65428302]\n",
      "  [ 0.64499516]\n",
      "  [ 0.63425477]\n",
      "  [ 0.6602694 ]\n",
      "  [ 0.503     ]]\n",
      "\n",
      " [[ 0.53838372]\n",
      "  [ 0.56492648]\n",
      "  [ 0.57047278]\n",
      "  [ 0.58926841]\n",
      "  [ 0.6804297 ]\n",
      "  [ 0.65428302]\n",
      "  [ 0.64499516]\n",
      "  [ 0.63425477]\n",
      "  [ 0.6602694 ]\n",
      "  [ 0.6755877 ]\n",
      "  [-0.796     ]]\n",
      "\n",
      " [[ 0.56492648]\n",
      "  [ 0.57047278]\n",
      "  [ 0.58926841]\n",
      "  [ 0.6804297 ]\n",
      "  [ 0.65428302]\n",
      "  [ 0.64499516]\n",
      "  [ 0.63425477]\n",
      "  [ 0.6602694 ]\n",
      "  [ 0.6755877 ]\n",
      "  [ 0.59424246]\n",
      "  [ 0.813     ]]\n",
      "\n",
      " [[ 0.57047278]\n",
      "  [ 0.58926841]\n",
      "  [ 0.6804297 ]\n",
      "  [ 0.65428302]\n",
      "  [ 0.64499516]\n",
      "  [ 0.63425477]\n",
      "  [ 0.6602694 ]\n",
      "  [ 0.6755877 ]\n",
      "  [ 0.59424246]\n",
      "  [ 0.58156526]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.58926841]\n",
      "  [ 0.6804297 ]\n",
      "  [ 0.65428302]\n",
      "  [ 0.64499516]\n",
      "  [ 0.63425477]\n",
      "  [ 0.6602694 ]\n",
      "  [ 0.6755877 ]\n",
      "  [ 0.59424246]\n",
      "  [ 0.58156526]\n",
      "  [ 0.62135754]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.6804297 ]\n",
      "  [ 0.65428302]\n",
      "  [ 0.64499516]\n",
      "  [ 0.63425477]\n",
      "  [ 0.6602694 ]\n",
      "  [ 0.6755877 ]\n",
      "  [ 0.59424246]\n",
      "  [ 0.58156526]\n",
      "  [ 0.62135754]\n",
      "  [ 0.60471875]\n",
      "  [ 0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train: {}\".format(X_train.shape))\n",
    "print(\"X_test: {}\".format(X_test.shape))\n",
    "print(\"y_train: {}\".format(y_train.shape))\n",
    "print(\"y_test: {}\".format(y_test.shape))\n",
    "\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model architecture using TensorFlow\n",
    "def model_create():\n",
    "    tf.random.set_seed(1234)\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=(X_train.shape[1], 1)),\n",
    "            tf.keras.layers.LSTM(units=70, activation=\"tanh\", return_sequences=True),\n",
    "            tf.keras.layers.LSTM(units=30, activation=\"tanh\", return_sequences=True),\n",
    "            tf.keras.layers.LSTM(units=10, activation=\"tanh\", return_sequences=False),\n",
    "            tf.keras.layers.Dense(units=1, activation=\"linear\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.mean_squared_error,\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert the normalization on the test target values (y_test)\n",
    "y_test = scaler.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to predict stock prices for the test data\n",
    "def predict(model, test):\n",
    "    predictions = model.predict(test)\n",
    "    predictions = scaler.inverse_transform(predictions.reshape(-1, 1)).reshape(-1, 1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test data\n",
    "def evaluate(predictions):\n",
    "    mae = mean_absolute_error(predictions, y_test)\n",
    "    mape = mean_absolute_percentage_error(predictions, y_test)\n",
    "    return mae, mape, (1 - mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform trial runs of the model and get average evaluation results\n",
    "def run_model(n):\n",
    "    total_mae = total_mape = total_acc = 0\n",
    "    for i in range(n):\n",
    "        model = model_create()\n",
    "        predictions = predict(model, X_test)\n",
    "        mae, mape, acc = evaluate(predictions)\n",
    "        total_mae += mae\n",
    "        total_mape += mape\n",
    "        total_acc += acc\n",
    "    return (total_mae / n), (total_mape / n), (total_acc / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 4s 9ms/step - loss: 0.0843\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0132\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0085\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0104\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0065\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0046\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0049\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0057\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0034\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0032\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0032\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0034\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0034\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0050\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0052\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0035\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0030\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0028\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0029\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0029\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0032\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0038\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0029\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0034\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0030\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0029\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0029\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0027\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0027\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0026\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0027\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0028\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0029\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0032\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0036\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0029\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0030\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0042\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0046\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0033\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0033\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0032\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0027\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0026\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0027\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0027\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0032\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0028\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0030\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0032\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0030\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0037\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0031\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0025\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0027\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0024\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0031\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0029\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0026\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0026\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0030\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0028\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0026\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0028\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0027\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0039\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0028\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0026\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0026\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0025\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0025\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0030\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0030\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0028\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0024\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0025\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0028\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0026\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0025\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0026\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0038\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0037\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0034\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0029\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0030\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0027\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0035\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0034\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0036\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0031\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0042\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0036\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0032\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0029\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0029\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0033\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0025\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0026\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0028\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0026\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "Mean Absolute Error = 14.64339204268022\n",
      "Mean Absolute Percentage Error = 0.06499709048646704%\n",
      "Accuracy = 0.9350029095135329\n"
     ]
    }
   ],
   "source": [
    "# Perform a single trial run of the model\n",
    "mae, mape, acc = run_model(1)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Mean Absolute Error = {mae}\")\n",
    "print(f\"Mean Absolute Percentage Error = {mape}%\")\n",
    "print(f\"Accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model for frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model to a file named 'lstm_model.h5' in the current directory\n",
    "model.save('lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.026584288288354796, 0.028226847422264445, 0.023385509413632155, 0.02022999917288304, 0.031166278326279495, 0.007824037546620866, 0.007996914207664907, 0.0028097569186611437, 0.0042362367158367276, 0.0, 0.0]\n",
      "customX_test.shape: (45, 11)\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "customy_test len: 45\n",
      "predictions len: 45\n"
     ]
    }
   ],
   "source": [
    "# Load the existing LSTM model from the file\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('lstm_model.h5')\n",
    "\n",
    "# Load the custom_test.csv file\n",
    "data = pd.read_csv(\"../../stocks_hist.csv\")\n",
    "\n",
    "# Prepare data for prediction\n",
    "stock_column = ['Close']\n",
    "\n",
    "# Normalize data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data['MinMax_Close'] = scaler.fit_transform(data[stock_column])\n",
    "\n",
    "# Prepare input features (X) and target values (y) for testing data\n",
    "sequence_length = 10\n",
    "customX_test = []\n",
    "customy_test = []\n",
    "related_stocks = []\n",
    "related_dates = []\n",
    "for i in range(len(data) - sequence_length):\n",
    "    related_date = data['Date'].values[i]\n",
    "    related_dates.append(related_date)\n",
    "    related_stock = data['ticker'].values[i]\n",
    "    related_stocks.append(related_stock)\n",
    "    customX_test.append(data['MinMax_Close'].values[i: i + sequence_length])\n",
    "    customy_test.append(data['MinMax_Close'].values[i + sequence_length])\n",
    "\n",
    "for i in range(len(customX_test)):\n",
    "    customX_test[i] = customX_test[i].tolist()\n",
    "    customX_test[i].append(0.0)\n",
    "    if i == 0:\n",
    "        print(customX_test[i])\n",
    "customX_test = np.array(customX_test).astype(float)\n",
    "\n",
    "# customX_test = np.array(customX_test)\n",
    "customy_test = np.array(customy_test)\n",
    "\n",
    "print(\"customX_test.shape: {}\".format(customX_test.shape))\n",
    "predictions = predict(model, customX_test)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "print(\"customy_test len: {}\".format(len(customy_test)))\n",
    "print(\"predictions len: {}\".format(len(predictions)))\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'Date': related_dates,\n",
    "    'Stock': related_stocks,\n",
    "    'Actual_Close': customy_test.flatten(),\n",
    "    'Predicted_Close': predictions.flatten()\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date Stock  Actual_Close  Predicted_Close  Real_Close\n",
      "0   2023-07-13  AMZN    128.250000       130.127472  134.300003\n",
      "1   2023-07-14  AMZN    313.410004       130.350464  134.679993\n",
      "2   2023-07-17  AMZN    308.869995       185.019608  133.559998\n",
      "3   2023-07-18  AMZN    310.619995       241.455811  132.830002\n",
      "4   2023-07-19  AMZN    312.049988       270.443207  135.360001\n",
      "5   2023-07-20  AMZN    316.010010       291.860748  129.960007\n",
      "6   2023-07-21  AMZN    302.519989       306.462738  130.000000\n",
      "7   2023-07-24  AMZN    294.260010       296.026093  128.800003\n",
      "8   2023-07-25  AMZN    291.609985       282.161530  129.130005\n",
      "9   2023-07-26  AMZN    294.470001       279.026886  128.149994\n",
      "10  2023-07-27  AMZN    298.570007       285.159363  128.250000\n",
      "11  2023-07-13  META    311.709991       293.933258  313.410004\n",
      "12  2023-07-14  META    190.539993       303.170135  308.869995\n",
      "13  2023-07-17  META    190.690002       167.113998  310.619995\n",
      "14  2023-07-18  META    193.990005       168.046463  312.049988\n",
      "15  2023-07-19  META    193.729996       178.266403  316.010010\n",
      "16  2023-07-20  META    195.100006       186.184296  302.519989\n",
      "17  2023-07-21  META    193.130005       193.351868  294.260010\n",
      "18  2023-07-24  META    191.940002       192.644379  291.609985\n",
      "19  2023-07-25  META    192.750000       191.050095  294.470001\n",
      "20  2023-07-26  META    193.619995       191.488541  298.570007\n",
      "21  2023-07-27  META    194.500000       192.802322  311.709991\n",
      "22  2023-07-13  AAPL    193.220001       193.167618  190.539993\n",
      "23  2023-07-14  AAPL    342.660004       191.985413  190.690002\n",
      "24  2023-07-17  AAPL    345.239990       235.239212  193.990005\n",
      "25  2023-07-18  AAPL    345.730011       281.861389  193.729996\n",
      "26  2023-07-19  AAPL    359.489990       305.480652  195.100006\n",
      "27  2023-07-20  AAPL    355.079987       325.273071  193.130005\n",
      "28  2023-07-21  AAPL    346.869995       340.398041  191.940002\n",
      "29  2023-07-24  AAPL    343.769989       330.923706  192.750000\n",
      "30  2023-07-25  AAPL    345.109985       319.783691  193.619995\n",
      "31  2023-07-26  AAPL    350.980011       320.192749  194.500000\n",
      "32  2023-07-27  AAPL    337.769989       329.467896  193.220001\n",
      "33  2023-07-13  MSFT    330.720001       329.329376  342.660004\n",
      "34  2023-07-14  MSFT    277.899994       315.214661  345.239990\n",
      "35  2023-07-17  MSFT    281.380005       250.883118  345.730011\n",
      "36  2023-07-18  MSFT    290.380005       251.382599  359.489990\n",
      "37  2023-07-19  MSFT    293.339996       269.502838  355.079987\n",
      "38  2023-07-20  MSFT    291.260010       284.039490  346.869995\n",
      "39  2023-07-21  MSFT    262.899994       289.516876  343.769989\n",
      "40  2023-07-24  MSFT    260.019989       262.583282  345.109985\n",
      "41  2023-07-25  MSFT    269.059998       251.505936  350.980011\n",
      "42  2023-07-26  MSFT    265.279999       260.732513  337.769989\n",
      "43  2023-07-27  MSFT    264.350006       263.621460  330.720001\n",
      "44  2023-07-13  TSLA    255.710007       262.783264  277.899994\n"
     ]
    }
   ],
   "source": [
    "transformed_column = result_df['Actual_Close'].values.reshape(-1, 1)  # Assuming 'transformed_column' is the column to be converted back\n",
    "original_column = scaler.inverse_transform(transformed_column)\n",
    "\n",
    "result_df['Actual_Close'] = original_column\n",
    "result_df['Real_Close'] = data['Close']\n",
    "\n",
    "print(result_df)\n",
    "\n",
    "result_df.to_csv(\"lol.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE) = 30.86593288845486\n",
      "Mean Absolute Percentage Error (MAPE) = 10.61%\n",
      "Accuracy within 5% threshold = 46.67%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def accuracy_within_threshold(y_true, y_pred, threshold):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true) <= threshold) * 100\n",
    "\n",
    "# Assuming 'result_df' contains the actual and predicted stock prices as columns 'Actual_Close' and 'Predicted_Close', respectively.\n",
    "actual_stock = result_df['Actual_Close'].values\n",
    "predicted_stock = result_df['Predicted_Close'].values\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(actual_stock, predicted_stock)\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = mean_absolute_percentage_error(actual_stock, predicted_stock)\n",
    "\n",
    "# Calculate Accuracy within a 5% threshold\n",
    "threshold = 0.05  # 5% threshold (you can adjust this value as needed)\n",
    "accuracy = accuracy_within_threshold(actual_stock, predicted_stock, threshold)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Mean Absolute Error (MAE) = {mae}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE) = {mape:.2f}%\")\n",
    "print(f\"Accuracy within {threshold*100:.0f}% threshold = {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
